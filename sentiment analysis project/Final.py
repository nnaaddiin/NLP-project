# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WkazZ-X3KiU4YYEoR5DBP47D474WJhre
"""

import os


import nltk
import sns
import spacy
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
# from nltk.corpus import stopwords
import re



positive_verbs = [
    "good",
    "great",
    "well",
    "wonderful",
    "develop",
    "optimize",
    "succeed",
    "better",
    "perfect",
    "excellent",
    "improve",
    "enhance",
    "boost",
    "advance",
    "ameliorate",
    "flourish",
    "thrive",
    "prosper",
    "excel",
    "refine",
    "evolve",
    "progress",
    "upturn",
    "enrich",
    "elevate",
    "polish",
    "fortify",
    "strengthen",
    "augment",
    "superb",
    "exceptional"
]
negative_verbs = [
    "bad",
    "fail",
    "lose",
    "ruin",
    "deteriorate",
    "degrade",
    "worsen",
    "decline",
    "deter",
    "impair",
    "hinder",
    "diminish",
    "weaken",
    "undermine",
    "damage",
    "destroy",
    "harm",
    "neglect",
    "negate",
    "invalidate",
    "obstruct",
    "inhibit",
    "restrain",
    "limit",
    "restrict",
    "constrain",
    "suppress",
    "hamper",
    "impede",
    "hamstring",
    "cripple",
    "paralyze",
    "stifle",
    "oppress",
    "trouble",
    "struggle",
    "suffer",
    "deteriorate",
    "regress",
    "fail"]


#nltk.download('stopwords')
#nltk.download('punkt')
#nltk.download('wordnet')
#nltk.download('all')

# print(stopwords)

liststop = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn',  'didn',  'doesn',  'hadn',  'hasn',  'haven', 'isn', 'ma', 'mightn',  'mustn',  'needn',  'shan',  'shouldn', 'wasn', 'weren', 'won', 'wouldn',]

positive_reviews = []
negative_reviews = []

# Load positive reviews
for filename in os.listdir('data/pos'):
    with open(os.path.join('data/pos', filename), 'r') as f:
        review = f.read()
        positive_reviews.append(review)

# Load negative reviews
for filename in os.listdir('data/neg'):
    with open(os.path.join('data/neg', filename), 'r') as f:
        review = f.read()
        negative_reviews.append(review)


def remove_stopwords(tokens):
    # stopwordsList = set(stopwords.words('english'))  # Example list of stopwords
    meaningful_words = {'not', 'no', 'but', 'none','never', 'nobody', 'neither', 'nothing', 'nowhere'}

    # for i in range(len(tokens)):
    #     if i > 0:
    #         if tokens[i - 1] == 'not':
    #             if tokens[i] in positive_verbs:
    #
    for i in range(1, len(tokens)):
        # Check if the current token is in the positive verbs list and the previous token is "not"
        if tokens[i] in positive_verbs and tokens[i - 1] == "not":
            tokens[i] = 'bad'
        elif tokens[i] in negative_verbs and tokens[i - 1] == "not":
            tokens[i] = 'good'


    filtered_tokens_temp = [word for word in tokens if word.lower() not in liststop or word in meaningful_words]
    return filtered_tokens_temp

def clean_text(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Keep only alphabetic characters and spaces
    text = text.lower().strip()
    return text


# Array of sentences
cleaned_texts_pos = []
cleaned_texts_neg = []


for text1 in positive_reviews:
    cleaned_texts_pos.append(clean_text(text1))

for text2 in negative_reviews:
    cleaned_texts_neg.append(clean_text(text2))

# Remove stopwords from each sentence and print the words
for i in range(len(cleaned_texts_pos)):
    tokens_pos = nltk.word_tokenize(cleaned_texts_pos[i])  # Tokenize the sentence into words
    filtered_tokens_pos = remove_stopwords(tokens_pos)
    cleaned_texts_pos[i] = ' '.join(filtered_tokens_pos)

for i in range(len(cleaned_texts_neg)):
    tokens_neg = nltk.word_tokenize(cleaned_texts_neg[i])  # Tokenize the sentence into words
    filtered_tokens_neg = remove_stopwords(tokens_neg)
    cleaned_texts_neg[i] = ' '.join(filtered_tokens_neg)

# print(cleaned_texts_pos)
# print(cleaned_texts_neg)
df = pd.DataFrame({
    'review': cleaned_texts_pos + cleaned_texts_neg,
    'label': [1] * len(cleaned_texts_pos) + [0] * len(cleaned_texts_neg)
})

# print(df['review'].head())  # Print the first few rows of the DataFrame
# print(df.shape)
# print(df['label'].value_counts())

lemmatizer = WordNetLemmatizer()
nlp = spacy.load('en_core_web_sm')

stemmer = PorterStemmer()

def apply_lemmatization(text):
    # doc = nlp(text)
    # lemmatized_tokens = [token.lemma_ for token in doc]
    # return ' '.join(lemmatized_tokens)
    words = nltk.word_tokenize(text)
    stemmed_words = [stemmer.stem(word) for word in words]
    return ' '.join(stemmed_words)


df['review'] = df['review'].apply(lambda x: apply_lemmatization(x))


from sklearn.feature_extraction.text import TfidfVectorizer

cv = TfidfVectorizer( max_features=9999)

X = cv.fit_transform(df['review'])

X_csr = X.tocsr()

X = X.toarray()


X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)


from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


from sklearn.svm import SVC
svm_model = SVC(kernel='linear')

svm_model.fit(X_train, y_train)

y_pred = svm_model.predict(X_test)

accuracysvm = accuracy_score(y_test, y_pred)
print("Accuracy SVM:", accuracysvm)


print("\nClassification Report SVM:")
print(classification_report(y_test, y_pred))



from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracyclf = clf.score(X_test, y_test)
print(f"Accuracy clf: {accuracyclf}")
print("\nClassification Report Random Forest:")
print(classification_report(y_test, y_pred))










import matplotlib.pyplot as plt

# Accuracy values for two models
accuracy_model1 = accuracysvm
accuracy_model2 = accuracyclf

# Labels for the models
labels = ['Model SVM', 'Model Random Forest']

# Accuracies for the models
accuracies = [accuracy_model1, accuracy_model2]

# Plot the bar chart
plt.figure(figsize=(8, 6))
plt.bar(labels, accuracies, color=['skyblue', 'orange'])
plt.ylim(0, 1)  # Set y-axis limit to 0-1 for accuracy percentage
plt.ylabel('Accuracy')
plt.title('Comparison of Model Accuracies')
plt.show()

#print(cv.get_feature_names())


df = pd.DataFrame.sparse.from_spmatrix(X_csr, columns=cv.get_feature_names())

# Get the mean TF-IDF values for each term and sort them
mean_tfidf = df.mean(axis=0).sort_values(ascending=False)

# Plot only the top N terms
top_n = 20  # Change this to plot more or fewer terms
plt.figure(figsize=(12, 6))
mean_tfidf[:top_n].plot(kind='bar')
plt.xlabel('Term')
plt.ylabel('Mean TF-IDF Value')
plt.title(f'Top {top_n} Mean TF-IDF Values for Each Term')
plt.xticks(rotation=45)
plt.show()


# import pickle
#
# with open("classification.pkl", "wb") as f:
#     pickle.dump(svm_model, f)
#     pickle.dump(cv, f)


# def predict_sentiment(input_text):
#     # Preprocess the text
#     cleaned_text = clean_text(input_text)
#
#     # Tokenize the entire cleaned text
#     tokens = nltk.word_tokenize(cleaned_text)
#
#     # Remove stopwords
#     filtered_tokens = remove_stopwords(tokens)
#
#     # Join the filtered tokens back into a single string
#     filtered_text = ' '.join(filtered_tokens)
#
#     # Vectorize the text
#     vectorized_text = cv.transform([filtered_text])
#
#     # Predict using the model
#     vectorized_text = vectorized_text.toarray()
#     prediction = svm_model.predict(vectorized_text)
# # This movie is so good i love it
#     return prediction[0]
# while True:
#     user_input = input("Enter text for sentiment analysis or type 'exit' to quit: ")
#     if user_input.lower() == 'exit':
#         print("Exiting the program.")
#         break
#     output = predict_sentiment(user_input)
#     if output == 1:
#         print(f"Predicted sentiment: Positive")
#     elif output == 0:
#         print(f"Predicted sentiment: Negative")
#     else:
#         print(f"Predicted sentiment: Wrong")



